{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e51743c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt, Kkma\n",
    "from Token import UserTokenizers\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d9f4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTransfromVect:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        info : 자연어를 벡터화하는 훈련과 데이터 변경을 수행하는 클래스\n",
    "        process : \n",
    "        1) self.fit_run 수행\n",
    "        2) self.transform_run 수행 \n",
    "      ----------------------\n",
    "      추가 개발 요구 사항\n",
    "      1) []벡터 훈련 모델 저장 코드 추가\n",
    "      2) [2023/05/12] 저장된 백터 훈련 모델 로드 코드 추가\n",
    "      3) []백터 변환 함수 수정 필요(훈련 모델 로드 코드)  \n",
    "        \"\"\"\n",
    "        self.ut_cls=UserTokenizers()\n",
    "\n",
    "    def fitWP_TFIDF(self):\n",
    "        \"\"\"\n",
    "        info : 훈련된 워드피스 토크나이저로 tfidf 벡터화 모델선언\n",
    "        \"\"\"        \n",
    "        self.wp_tfidf= TfidfVectorizer(tokenizer=self.ut_cls.WordPieceTokenizer)\n",
    "        return self.wp_tfidf   \n",
    "    def fitBPE_TFIDF(self):\n",
    "        \"\"\"\n",
    "        info : 훈련된 BPE 토크나이저로 tfidf 백터화 모델선언\n",
    "        주의 : 토크나이저 훈련 모델에서 <unk>문제 해결 필요\n",
    "        \"\"\"\n",
    "        self.bpe_tfidf = TfidfVectorizer(tokenizer= self.ut_cls.BPETokenizer)\n",
    "        return self.bpe_tfidf\n",
    "    def fitKKMA_TFIDF(self):\n",
    "        \"\"\"\n",
    "        info : 훈련된 Konlpy.Kkma 토크나이저로 tfidf 백터화 모델선언\n",
    "        사용주의: 메모리에러, 소요시간 주의\n",
    "        \"\"\"\n",
    "        self.kkma_tfidf = TfidfVectorizer(tokenizer=self.ut_cls.konlpyNounsTokenizer)\n",
    "        \n",
    "        return self.kkma_tfidf\n",
    "    def fitMP_TFIDF(self):\n",
    "        \"\"\"\n",
    "        info : 훈련된 konlpy.Okt 토크나이저로 tfidf 벡터화 모델선언\n",
    "        사용주의: 메모리에러, 소요시간 주의\n",
    "        \"\"\"\n",
    "        self.mp_tfidf= TfidfVectorizer(tokenizer=self.ut_cls.konlyMorphsTokenizer)\n",
    "        \n",
    "        return self.mp_tfidf\n",
    "    \n",
    "    def fit_run(self, user_token_nm,data) :\n",
    "        \"\"\"\n",
    "        info : tfidf 벡터화 수행 \n",
    "        param user_token_nm : 토크나이저 선택 {'wp': '워드피스','bpe':'BPE','kkma':'꼬꼬마','mp':'Okt'}\n",
    "        param data:백터 모델 훈련 데이터\n",
    "        \"\"\" \n",
    "        if user_token_nm =='wp':\n",
    "            self.vec_model= self.fitWP_TFIDF()\n",
    "            # 모델 저장 코드 추가 필요\n",
    "            \n",
    "        elif user_token_nm =='bpe':\n",
    "            self.vec_model= self.fitBPE_TFIDF()\n",
    "            # 모델 저장 코드 추가 필요\n",
    "        elif user_token_nm =='kkma':\n",
    "            self.vec_model= self.fitKKMA_TFIDF()\n",
    "            # 모델 저장 코드 추가 필요\n",
    "        elif user_token_nm =='mp':\n",
    "            self.vec_model= self.fitMP_TFIDF()\n",
    "            # 모델 저장 코드 추가 필요\n",
    "            \n",
    "        else :\n",
    "            # 모델 개발 할 때 혹은 인수인계자에게 디버깅하기 좋은 코드로 넘길 때 사용\n",
    "            raise ValueError(\"user_token_nm이 올바르지 않습니다.['wp','bpe','kkma','mp'] \")\n",
    "            # 서비스 운영을 할 때 기본설정하고, logging을 이용해서 로그를 남기거나 슬랙 알람/ 메일링 주는 모듈이 실행되게 함\n",
    "            # self.vec_model = self.fitWP_TFIDF()\n",
    "        self.vec_model.fit(data)\n",
    "        ## 저장된 모델이 있는 경우 : 모델을 불러오는 모듈로 전환\n",
    "    \n",
    "    def transform_run(self, data, chunk_size):\n",
    "        \"\"\"\n",
    "        info: tfidf 백터화 하여 np.array로 변환\n",
    "        param data : 벡터로 변환하려는 데이터\n",
    "        param chunk_size : np.array로 변환하는 단위, 데이터 수\n",
    "        return vec_arr: np.array \n",
    "        \"\"\"\n",
    "        ## 데이터 수를 조정해서 데이터를 변환함\n",
    "        data_len= len(data)\n",
    "        for st_idx in tqdm(range(0, data_len, chunk_size)):\n",
    "            tmp_data = data[st_idx:st_idx+chunk_size]\n",
    "            \n",
    "            if st_idx==0:\n",
    "                vac_arr= self.vec_model.transform(tmp_data).toarray()\n",
    "            else :\n",
    "                tmp_data_arr= self.vec_model.transform(tmp_data).toarray()\n",
    "                vac_arr= np.append(vac_arr,tmp_data_arr, axis=0)\n",
    "        return vac_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28950920",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttfv=TrainTransfromVect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be6dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('./ratings_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fdaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5125620a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ttfv.fit_run('wp', train_df['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e3992c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(tokenizer=&lt;bound method UserTokenizers.WordPieceTokenizer of &lt;Token.UserTokenizers object at 0x7f2ded05bf70&gt;&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(tokenizer=&lt;bound method UserTokenizers.WordPieceTokenizer of &lt;Token.UserTokenizers object at 0x7f2ded05bf70&gt;&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(tokenizer=<bound method UserTokenizers.WordPieceTokenizer of <Token.UserTokenizers object at 0x7f2ded05bf70>>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttfv.vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b614ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 858.52it/s]\n"
     ]
    }
   ],
   "source": [
    "wp_tfidf_vec=ttfv.transform_run(train_df['document'][:200],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ca89653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 9873)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tfidf_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121a9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_preprocessing import TrainTransfromVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a1413cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ttfv=TrainTransfromVect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eaf7873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_ttfv.fit_run('wp', train_df['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bd002d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_ttfv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_wp_tfidf_vec \u001b[38;5;241m=\u001b[39m \u001b[43mnew_ttfv\u001b[49m\u001b[38;5;241m.\u001b[39mtransform_run(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m10000\u001b[39m],chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_ttfv' is not defined"
     ]
    }
   ],
   "source": [
    "new_wp_tfidf_vec = new_ttfv.transform_run(train_df['document'][:10000],chunk_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b862f910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_ttfv.fit_run('bpe',train_df['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88ac6ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 69.10it/s]\n"
     ]
    }
   ],
   "source": [
    "ne_bpe_tfidf_vec = new_ttfv.transform_run(train_df['document'][:1000], chunk_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "784e9510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_ttfv.fit_run('kkma',train_df['document'][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "004fd88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.25it/s]\n"
     ]
    }
   ],
   "source": [
    "new_kkma_tfidf_vec=new_ttfv.transform_run_train_dfsform_run(train_df['document'][:100],chunk_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e841bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_ttfv.fit_run('mp',train_df['document'][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "492f00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 102.15it/s]\n"
     ]
    }
   ],
   "source": [
    "new_mp_tfidf_vec=new_ttfv.transform_run(train_df['document'][:100],chunk_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77b13648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mp_tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b5c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
